# -*- coding: utf-8 -*-
"""Submission predictive analytics fix.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xBRBLOd3XiuFjaXgnYnYPObEtnrKsZ9M

# Import lib yang dibutuhkan
Untuk analysis memerlukan beberapa library ini untuk menganalisis data sebelum dilakukan proses modelling
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
# %matplotlib inline
import seaborn as sns

"""# Take dataset
Untuk kali ini saya akan menggunakan dataset Dummy marketing and sales Data, yang berasal dari kaggle dengan link https://www.kaggle.com/datasets/harrimansaragih/dummy-advertising-and-sales-data/data
"""

import kagglehub

# Download latest version
path = kagglehub.dataset_download("harrimansaragih/dummy-advertising-and-sales-data")

print("Path to dataset files:", path)

"""Pindahkan file yang telah didownload ke directory yang sedang kita gunakan

"""

import os
# Create the target directory if it doesn't exist
os.makedirs("./files", exist_ok=True)

# Move the files from the source to the target directory
!mv  /root/.cache/kagglehub/datasets/harrimansaragih/dummy-advertising-and-sales-data/versions/1/* ./files
print("Files moved to ./files directory")

"""gunakan pandas dataframe untuk mengambil data

# lakukan pengecekan data
"""

sales=pd.read_csv('files/Dummy Data HSS.csv')

sales.info()

sales.describe()

sales.head()

"""# Lakukan pengecekkan untuk mencari data yang kosong dan duplikat"""

sales.isna().sum()

sales.duplicated().sum()

"""Sepertinya tidak ada data yang kosong dan duplikat

Selanjutnya kita akan mencari data yang mempunyai value 0
"""

def count_zeros(column):
    return (column == 0).sum()

# Aplikasikan fungsi pada seluruh kolom dan tampilkan hasilnya
zero_counts = sales.apply(count_zeros)
print(zero_counts)

"""Dengan adanya beberapa sel yang memiliki value NA, kita dapat menghapus sel tersebut"""

sales.dropna(inplace=True)

sales.isna().sum()

"""Dengan begini maka kita bisa tetap melakukan analisis data. Selanjutnya kita harus melakukan identifikasi outlier"""

sns.boxplot(x=sales['Social Media'])

"""Di social media ada beberapa outlier yang perlu ditangani, oleh karena itu kita harus menanganinya dengan:"""

Q1 = sales.quantile(0.25,numeric_only=True)
Q3 = sales.quantile(0.75,numeric_only=True)
IQR = Q3 - Q1

# Create a boolean mask that will indicate the rows to keep
mask = pd.Series(True, index=sales.index) # Initialize mask with all True

for col in sales.columns:
    if pd.api.types.is_numeric_dtype(sales[col]): #check if column is numeric
        mask = mask & ~((sales[col] < (Q1[col] - 1.5 * IQR[col])) | (sales[col] > (Q3[col] + 1.5 * IQR[col]))) # apply the filtering condition per column

sales=sales[mask]
sales.shape

sns.boxplot(x=sales['Social Media'])

"""Bisa dilihat bahwa outlier telah ditangani

# EDA
## Univariate analysis
univariate analysis adalah teknik yang digunakan untuk analysis data secara 1 per 1 tanpa melihat korelasi antar data
"""

numerical_features = ['TV', 'Radio', 'Social Media', 'Sales']
categorical_features = ['Influencer']

"""### Categorical Feature"""

feature = categorical_features[0]
count = sales[feature].value_counts()
percent = 100*sales[feature].value_counts(normalize=True)
df = pd.DataFrame({'jumlah sampel':count, 'persentase':percent.round(1)})
print(df)
count.plot(kind='bar', title=feature);

"""Bisa dilihat dengan ketiga influencer hampir memliki jumlah data yang sama

Numerical Feature
"""

sales.hist(bins=50, figsize=(20,15))
plt.show()

"""## Multivariate analysis

### Categorial analysis
"""

cat_features = sales.select_dtypes(include='object').columns.to_list()

for col in cat_features:
  sns.catplot(x=col, y="Sales", kind="bar", dodge=False, height = 4, aspect = 3,  data=sales, palette="Set3")
  plt.title("Rata-rata 'price' Relatif terhadap - {}".format(col))

"""Bisa dilihat bahwa influencer: mega, micro, nano, dan macro hampir memiliki jumlah sales yang sama

### Numerical analysis
"""

sns.pairplot(sales, diag_kind = 'kde')

"""**Correlation matrix untuk fitur numerik**

correlation matrix berikisar 1 sampai -1
*   mendekati 1 artinya berkorelasi naik
*   mendekati 0 artinya tidak berkorelasi
*   mendekati -1 artinya berkorelasi turun
"""

plt.figure(figsize=(10, 8))
correlation_matrix = sales[numerical_features].corr().round(2)

# Untuk menge-print nilai di dalam kotak, gunakan parameter anot=True
sns.heatmap(data=correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5, )
plt.title("Correlation Matrix untuk Fitur Numerik ", size=20)

"""Bisa dilihat dari beberapa numerical feature yang ada memang memiliki korelasi satu sama lain, meskipun social media merupakan yang terlemah

#Encoding fitur kategori
agar algo ML berjalan dengan lancar maka, kita memerlukan untuk melakukan encoding untuk kategori
"""

# from sklearn.preprocessing import  OneHotEncoder
sales = pd.concat([sales, pd.get_dummies(sales['Influencer'], prefix='Influencer')],axis=1)
sales.drop(['Influencer'], axis=1, inplace=True)
sales.head()

"""# Reduksi dengan PCA
Sepertinya reduksi dengan metode PCA tidak perlu dilakukan karena fitur numerik sudah sedikit jadi tidak perlu direduksi kembali

# Train - Test Split

Untuk train-test split kita menggunakan split 90:10. Yang berarti data training 90% dan data test 10%
"""

from sklearn.model_selection import train_test_split

X = sales.drop(["Sales"],axis =1) #data
y = sales["Sales"] #label
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state = 123)

print(f'Total # of sample in whole dataset: {len(X)}')
print(f'Total # of sample in train dataset: {len(X_train)}')
print(f'Total # of sample in test dataset: {len(X_test)}')

"""
# Standarization
digunakan untuk algo ML memiliki peforma yang lebih baik dan lebih cepat ketika training. Hanya digunakan untuk fitur numerik. Menggunakan teknik StandarScaler dari lib scikitlearn.StandardScalaer mengurangkan mean, kemudian membaginya dengan standar deviasa untuk menggeser distribusi, maka nilai akan berada di antara -1 dan 1.Hanya dilakukan pada data training. Pada tahap evaluasi, kita akan melakukan standarisasi pada data uji."""

X_train.head()

from sklearn.preprocessing import StandardScaler

numerical_features = ['TV', 'Radio', 'Social Media']
scaler = StandardScaler()
scaler.fit(X_train[numerical_features])
X_train[numerical_features] = scaler.transform(X_train.loc[:, numerical_features])
X_train[numerical_features].head()

X_train.describe().round(4)

"""Dengan begini maka data siap untuk ditrain untuk mendapatkan hasil yang maksimal

# Model Development
Pada model development ini kita menggunakan 3 algoritma. Lalu menentukan mana yang memberikan hasil prediksi terbaik, yang digunakan:
*   K-Nearest Neighbor
*   Random Forest
*   Boosting Algo

Menyiapkan DF untuk analisis model
"""

# Siapkan dataframe untuk analisis model
models = pd.DataFrame(index=['train_mse', 'test_mse'],
                      columns=['KNN', 'RandomForest', 'Boosting'])

"""KNN"""

from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_squared_error

knn = KNeighborsRegressor(n_neighbors=10)
#n_neighbor berarti berapa neighbor yang dipakai untuk mencari jarak
knn.fit(X_train, y_train)

models.loc['train_mse','knn'] = mean_squared_error(y_pred = knn.predict(X_train), y_true=y_train)

"""RF"""

# Impor library yang dibutuhkan
from sklearn.ensemble import RandomForestRegressor
 #Perhatikanlah potongan kode di atas.
 # Pertama, Anda mengimpor RandomForestRegressor dari library scikit-learn.
 #Anda juga mengimpor mean_squared_error sebagai metrik untuk mengevaluasi performa model.

# buat model prediksi
RF = RandomForestRegressor(n_estimators=50, max_depth=16, random_state=55, n_jobs=-1)
RF.fit(X_train, y_train)

models.loc['train_mse','RandomForest'] = mean_squared_error(y_pred=RF.predict(X_train), y_true=y_train)

"""Boosting (adaboost)"""

from sklearn.ensemble import AdaBoostRegressor

boosting = AdaBoostRegressor(learning_rate=0.05, random_state=55)
boosting.fit(X_train, y_train)
models.loc['train_mse','Boosting'] = mean_squared_error(y_pred=boosting.predict(X_train), y_true=y_train)

"""# Evaluasi Model

kita akan menggunakan mse

Pertama kita akan melakukan standarisasi pada data tes
"""

# Lakukan scaling terhadap fitur numerik pada X_test sehingga memiliki rata-rata=0 dan varians=1
X_test.loc[:, numerical_features] = scaler.transform(X_test[numerical_features])

X_test.describe()

"""Selanjutnya kita lakukan evaluasi"""

# Buat variabel mse yang isinya adalah dataframe nilai mse data train dan test pada masing-masing algoritma
mse = pd.DataFrame(columns=['train', 'test'], index=['KNN','RF','Boosting'])

# Buat dictionary untuk setiap algoritma yang digunakan
model_dict = {'KNN': knn, 'RF': RF, 'Boosting': boosting}

# Hitung Mean Squared Error masing-masing algoritma pada data train dan test
for name, model in model_dict.items():
    mse.loc[name, 'train'] = mean_squared_error(y_true=y_train, y_pred=model.predict(X_train))/1e3
    mse.loc[name, 'test'] = mean_squared_error(y_true=y_test, y_pred=model.predict(X_test))/1e3

# Panggil mse
mse

fig, ax = plt.subplots()
mse.sort_values(by='test', ascending=False).plot(kind='barh', ax=ax, zorder=3)
ax.grid(zorder=0)

prediksi = X_test.iloc[:1000].copy()
pred_dict = {'y_true':y_test[:1000]}
for name, model in model_dict.items():
    pred_dict['prediksi_'+name] = model.predict(prediksi).round(1)

pd.DataFrame(pred_dict)

"""Bedasarkan 3 algo ML yang memiliki score paling baik adalah Boosting"""